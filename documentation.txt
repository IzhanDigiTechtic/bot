# USPTO Controller-Based Data Processing System Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Project Structure](#project-structure)
3. [Architecture](#architecture)
4. [Controller Components](#controller-components)
5. [Data Flow](#data-flow)
6. [Database Schema](#database-schema)
7. [Configuration](#configuration)
8. [Usage Examples](#usage-examples)
9. [Performance Optimization](#performance-optimization)
10. [Error Handling](#error-handling)
11. [Troubleshooting](#troubleshooting)
12. [Setup and Installation](#setup-and-installation)

---

## System Overview

The USPTO Controller-Based Data Processing System is a modular, scalable solution for downloading, processing, and storing USPTO trademark data. It separates concerns into distinct controllers that can work independently or be orchestrated together for maximum efficiency.

### Key Features
- **Modular Architecture**: Separate controllers for different responsibilities
- **Product-Specific Tables**: Dynamic table creation for each USPTO product
- **Batch Processing**: Memory-efficient processing of large datasets
- **Checkpoint/Resume**: Ability to resume interrupted processing
- **Optimized Storage**: Fast database operations with COPY optimization
- **Low-Spec Friendly**: Designed to work efficiently on limited hardware

---

## Project Structure

The USPTO Controller-Based Data Processing System is organized into a clean, modular structure that separates concerns and makes the system easy to maintain and extend.

### Directory Structure

```
📁 Project Root
├── 🚀 run_uspto.py                    # Main entry point
├── ⚙️ uspto_config.json               # Configuration file
├── 🛠️ setup.py                       # Setup script
├── 📋 requirements.txt                # Python dependencies
├── 🗄️ create_multi_product_schema.sql # Database schema
├── 📚 documentation.txt               # Complete documentation
├── 📖 README.md                       # Project overview
├── 📁 controllers/                    # Controller package
│   ├── __init__.py                    # Package initialization
│   ├── 📁 core/                       # Core controllers
│   │   ├── __init__.py
│   │   ├── uspto_controllers.py       # Main controller classes
│   │   └── uspto_controller_runner.py # Runner and configuration
│   ├── 📁 examples/                   # Usage examples
│   │   ├── __init__.py
│   │   └── controller_examples.py     # Example scripts
│   └── 📁 utils/                      # Utility scripts
│       ├── __init__.py
│       └── check_tables.py            # Database verification
└── 📊 uspto_data/                     # Data directory
    ├── zips/                          # Downloaded files
    ├── extracted/                     # Extracted files
    ├── processed/                     # Processed data
    ├── checkpoints/                   # Processing checkpoints
    └── batches/                       # Batch files
```

### Package Organization

#### Core Controllers (`controllers/core/`)
Contains the main controller classes that handle different aspects of data processing:
- **USPTOAPIController**: Handles USPTO API interactions
- **DownloadController**: Manages file downloads and extraction
- **ProcessingController**: Transforms raw data into structured format
- **DatabaseController**: Handles database operations and optimization
- **USPTOOrchestrator**: Coordinates all controllers into a unified process

#### Examples (`controllers/examples/`)
Contains example scripts showing how to use the controller system:
- **controller_examples.py**: Comprehensive usage examples
- Individual controller usage examples
- Custom workflow examples
- Error handling examples

#### Utilities (`controllers/utils/`)
Contains utility scripts for system management:
- **check_tables.py**: Database verification and monitoring
- System health checks
- Performance monitoring tools

### Import System

The package structure allows for easy imports:

```python
# Import from controllers package
from controllers.core.uspto_controllers import USPTOOrchestrator
from controllers.examples.controller_examples import example_orchestrator
from controllers.utils.check_tables import check_tables

# Or import specific controllers
from controllers.core.uspto_controllers import (
    USPTOAPIController,
    DownloadController,
    ProcessingController,
    DatabaseController
)
```

---

## Architecture

### High-Level Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                    USPTO Orchestrator                      │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────┐ │
│  │     API     │ │  Download   │ │ Processing  │ │Database │ │
│  │ Controller  │ │ Controller  │ │ Controller  │ │Controller│ │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────┘ │
└─────────────────────────────────────────────────────────────┘
           │              │              │              │
           ▼              ▼              ▼              ▼
    USPTO API        File System    Data Transform   PostgreSQL
```

### Component Relationships
- **Orchestrator**: Coordinates all controllers and manages the overall process
- **API Controller**: Handles USPTO API interactions
- **Download Controller**: Manages file downloads and extraction
- **Processing Controller**: Transforms raw data into structured format
- **Database Controller**: Handles database operations and optimization

---

## Controller Components

### 1. USPTOAPIController

**Purpose**: Manages all interactions with the USPTO API

**Key Methods**:
- `get_trademark_datasets()`: Fetches available trademark datasets
- `check_file_status()`: Checks if files need downloading/processing

**How It Works**:
1. Establishes HTTP session with proper headers
2. Makes API calls to USPTO endpoints
3. Parses JSON responses into structured data
4. Handles API errors and retries
5. Returns ProductInfo objects with file details

**Example Usage**:
```python
api_controller = USPTOAPIController(config)
api_controller.initialize()
products = api_controller.get_trademark_datasets()
```

### 2. DownloadController

**Purpose**: Handles file downloads and ZIP extraction

**Key Methods**:
- `download_file()`: Downloads files with progress tracking
- `extract_zip_file()`: Extracts ZIP archives
- `find_data_files()`: Locates CSV/XML files in directories

**How It Works**:
1. Creates product-specific directories
2. Checks for existing files to avoid re-downloads
3. Streams downloads with progress reporting
4. Verifies file integrity after download
5. Extracts ZIP files when needed

**Example Usage**:
```python
download_controller = DownloadController(config)
file_path = download_controller.download_file(file_info)
```

### 3. ProcessingController

**Purpose**: Transforms raw data into structured format

**Key Methods**:
- `process_csv_file()`: Processes CSV files in batches
- `process_xml_file()`: Processes XML files in batches
- `_clean_record()`: Cleans and normalizes data records

**How It Works**:
1. Reads files in configurable chunks to manage memory
2. Cleans column names and data values
3. Normalizes data types and handles null values
4. Adds metadata (data_source, batch_number)
5. Yields processed batches for database insertion

**Example Usage**:
```python
processing_controller = ProcessingController(config)
for batch in processing_controller.process_csv_file(file_path, product_id):
    # Process each batch
```

### 4. DatabaseController

**Purpose**: Manages database operations and schema creation

**Key Methods**:
- `register_product()`: Registers products and creates tables
- `save_batch()`: Saves processed batches to database
- `_create_product_table()`: Creates product-specific tables

**How It Works**:
1. Creates control tables (uspto_products, file_processing_history)
2. Dynamically creates product-specific tables based on data type
3. Uses optimized batch inserts with COPY operations
4. Manages indexes and constraints
5. Tracks processing statistics

**Example Usage**:
```python
database_controller = DatabaseController(config)
database_controller.register_product(product_info)
result = database_controller.save_batch(batch_data, product_id, batch_number)
```

### 5. USPTOOrchestrator

**Purpose**: Coordinates all controllers into a unified process

**Key Methods**:
- `initialize()`: Initializes all controllers
- `run_full_process()`: Executes the complete pipeline
- `cleanup()`: Cleans up all resources

**How It Works**:
1. Initializes all controllers in sequence
2. Executes the complete data processing pipeline
3. Handles errors and provides recovery mechanisms
4. Manages resource cleanup
5. Returns comprehensive processing results

**Example Usage**:
```python
orchestrator = USPTOOrchestrator(config)
orchestrator.initialize()
results = orchestrator.run_full_process()
orchestrator.cleanup()
```

---

## Data Flow

### Complete Processing Pipeline

```
1. API Controller
   ├── Fetch available datasets from USPTO API
   ├── Parse JSON responses into ProductInfo objects
   └── Return list of products with file information

2. Database Controller
   ├── Register each product in uspto_products table
   ├── Create product-specific table (e.g., product_trcfeco2)
   └── Set up indexes and constraints

3. Download Controller
   ├── Download files to product-specific directories
   ├── Verify file integrity and size
   └── Extract ZIP files if needed

4. Processing Controller
   ├── Read files in configurable chunks
   ├── Clean and normalize data
   ├── Add metadata (data_source, batch_number)
   └── Yield processed batches

5. Database Controller
   ├── Receive processed batches
   ├── Insert data using optimized batch operations
   ├── Update processing statistics
   └── Handle conflicts and errors

6. Orchestrator
   ├── Coordinate all steps
   ├── Handle errors and recovery
   ├── Track overall progress
   └── Return comprehensive results
```

### Memory Management Flow

```
Large File → Chunk Reading → Batch Processing → Database Insert → Memory Cleanup
     ↓              ↓              ↓              ↓              ↓
  Stream File → Process Chunk → Clean Data → Save Batch → Garbage Collect
```

---

## Database Schema

### Control Tables

#### uspto_products
```sql
CREATE TABLE uspto_products (
    id SERIAL PRIMARY KEY,
    product_id VARCHAR(50) UNIQUE,
    title TEXT,
    description TEXT,
    frequency VARCHAR(20),
    from_date DATE,
    to_date DATE,
    total_size BIGINT,
    file_count INTEGER,
    last_modified TIMESTAMP,
    formats TEXT[],
    table_name VARCHAR(50),
    schema_created BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### file_processing_history
```sql
CREATE TABLE file_processing_history (
    id SERIAL PRIMARY KEY,
    product_id VARCHAR(50),
    file_name VARCHAR(255),
    file_url TEXT,
    file_size BIGINT,
    file_hash VARCHAR(64),
    download_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_started TIMESTAMP,
    processing_completed TIMESTAMP,
    rows_processed INTEGER DEFAULT 0,
    rows_saved INTEGER DEFAULT 0,
    status VARCHAR(20) DEFAULT 'pending',
    error_message TEXT,
    processing_attempts INTEGER DEFAULT 0,
    batch_count INTEGER DEFAULT 0,
    last_batch_processed INTEGER DEFAULT 0,
    UNIQUE(product_id, file_name)
);
```

### Product-Specific Tables

#### Case File Data (TRCFECO products)
```sql
CREATE TABLE product_trcfeco2 (
    id SERIAL PRIMARY KEY,
    serial_number VARCHAR(20) UNIQUE,
    registration_number VARCHAR(20),
    filing_date DATE,
    registration_date DATE,
    status_code VARCHAR(10),
    status_date DATE,
    mark_identification TEXT,
    mark_drawing_code VARCHAR(10),
    -- ... many more fields ...
    data_source VARCHAR(100),
    file_hash VARCHAR(64),
    batch_number INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Assignment Data (TRASECO products)
```sql
CREATE TABLE product_traseco (
    id SERIAL PRIMARY KEY,
    assignment_id VARCHAR(50) UNIQUE,
    serial_number VARCHAR(20),
    registration_number VARCHAR(20),
    date_recorded DATE,
    conveyance_text TEXT,
    frame_no VARCHAR(10),
    reel_no VARCHAR(10),
    -- ... more fields ...
    data_source VARCHAR(100),
    file_hash VARCHAR(64),
    batch_number INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## Configuration

### Configuration File Structure

The system uses a JSON configuration file (`uspto_config.json`):

```json
{
  "api": {
    "api_url": "https://data.uspto.gov/ui/datasets/products/search",
    "timeout": 30,
    "retry_attempts": 3
  },
  "download": {
    "download_dir": "./uspto_data",
    "keep_latest_zips": 10,
    "force_redownload": false
  },
  "processing": {
    "batch_size": 10000,
    "chunk_size": 50000,
    "memory_limit_mb": 512,
    "max_workers": 2
  },
  "database": {
    "dbname": "trademarks",
    "user": "postgres",
    "password": "1234",
    "host": "localhost",
    "port": "5432",
    "use_copy": true
  },
  "orchestrator": {
    "max_files_per_product": 2,
    "enable_parallel_processing": false,
    "log_level": "INFO"
  }
}
```

### Configuration Parameters

#### API Configuration
- `api_url`: USPTO API endpoint URL
- `timeout`: Request timeout in seconds
- `retry_attempts`: Number of retry attempts for failed requests

#### Download Configuration
- `download_dir`: Base directory for downloaded files
- `keep_latest_zips`: Number of latest ZIP files to keep
- `force_redownload`: Force redownload of existing files

#### Processing Configuration
- `batch_size`: Number of records per batch
- `chunk_size`: Number of rows to read at once
- `memory_limit_mb`: Memory limit in megabytes
- `max_workers`: Maximum number of worker processes

#### Database Configuration
- `dbname`: Database name
- `user`: Database username
- `password`: Database password
- `host`: Database host
- `port`: Database port
- `use_copy`: Use COPY operations for faster inserts

#### Orchestrator Configuration
- `max_files_per_product`: Maximum files to process per product
- `enable_parallel_processing`: Enable parallel processing
- `log_level`: Logging level (DEBUG, INFO, WARNING, ERROR)

---

## Usage Examples

### Basic Usage

```bash
# Run with default settings
python run_uspto.py

# Process only 1 file per product
python run_uspto.py --max-files 1

# Force redownload all files
python run_uspto.py --force-redownload
```

### Advanced Usage

```bash
# Test all controllers
python run_uspto.py --test

# Optimize for low-memory system
python run_uspto.py --batch-size 5000 --memory-limit 256

# Process specific product
python run_uspto.py --product-id TRCFECO2

# Use custom configuration
python run_uspto.py --config my_config.json
```

### Programmatic Usage

#### Using Individual Controllers
```python
from controllers.core.uspto_controllers import USPTOAPIController, DatabaseController

# Initialize controllers
api_controller = USPTOAPIController(config)
database_controller = DatabaseController(config)

# Use controllers
api_controller.initialize()
products = api_controller.get_trademark_datasets()

database_controller.initialize()
for product in products:
    database_controller.register_product(product)

# Cleanup
api_controller.cleanup()
database_controller.cleanup()
```

#### Using the Orchestrator
```python
from controllers.core.uspto_controllers import USPTOOrchestrator

# Create orchestrator
orchestrator = USPTOOrchestrator(config)

# Run full process
orchestrator.initialize()
results = orchestrator.run_full_process(max_files_per_product=2)
orchestrator.cleanup()

# Check results
print(f"Success: {results['success']}")
print(f"Rows processed: {results['total_rows_processed']}")
print(f"Rows saved: {results['total_rows_saved']}")
```

---

## Performance Optimization

### Memory Management

The system is designed for efficient memory usage:

1. **Chunked Reading**: Files are read in configurable chunks
2. **Batch Processing**: Data is processed in small batches
3. **Garbage Collection**: Memory is cleaned up between batches
4. **Streaming**: Large files are streamed rather than loaded entirely

### Database Optimization

1. **Batch Inserts**: Uses `execute_values` for fast batch inserts
2. **COPY Operations**: Optional COPY operations for maximum speed
3. **Indexes**: Automatic index creation on key columns
4. **Connection Pooling**: Efficient database connection management

### Processing Optimization

1. **Parallel Processing**: Optional parallel processing for multiple files
2. **Progress Tracking**: Real-time progress reporting
3. **Error Recovery**: Graceful handling of processing errors
4. **Checkpoint/Resume**: Ability to resume interrupted processing

### Low-Spec System Optimization

For systems with limited resources:

```bash
# Reduce batch size and memory usage
python run_uspto.py --batch-size 5000 --memory-limit 256

# Process fewer files at once
python run_uspto.py --max-files 1

# Use smaller chunk sizes
# Edit config file: "chunk_size": 25000
```

---

## Error Handling

### Error Types and Handling

#### API Errors
- **Network Timeouts**: Automatic retry with exponential backoff
- **HTTP Errors**: Detailed error logging and graceful degradation
- **Rate Limiting**: Automatic throttling and retry

#### Download Errors
- **File Corruption**: Automatic re-download
- **Size Mismatch**: Verification and re-download
- **Network Issues**: Retry with progress preservation

#### Processing Errors
- **Memory Issues**: Automatic batch size reduction
- **Data Corruption**: Skip corrupted records and continue
- **Format Errors**: Detailed error logging and recovery

#### Database Errors
- **Connection Issues**: Automatic reconnection
- **Constraint Violations**: Skip duplicate records
- **Transaction Errors**: Rollback and retry

### Error Recovery Mechanisms

1. **Checkpoint System**: Save progress at regular intervals
2. **Resume Capability**: Restart from last checkpoint
3. **Error Logging**: Comprehensive error logging
4. **Graceful Degradation**: Continue processing despite errors

### Error Monitoring

```python
# Check processing results
results = orchestrator.run_full_process()

if not results['success']:
    print("Processing failed")
    for error in results['errors']:
        print(f"Error: {error}")
```

---

## Troubleshooting

### Common Issues

#### Database Connection Issues
**Problem**: Cannot connect to database
**Solution**: 
1. Check database credentials in config file
2. Ensure PostgreSQL is running
3. Verify database exists
4. Check network connectivity

#### Memory Issues
**Problem**: Out of memory errors
**Solution**:
1. Reduce `batch_size` in configuration
2. Lower `memory_limit_mb`
3. Process fewer files at once
4. Increase system memory

#### Download Failures
**Problem**: Files fail to download
**Solution**:
1. Check internet connection
2. Verify USPTO API accessibility
3. Use `--force-redownload` to retry
4. Check disk space

#### Processing Errors
**Problem**: Data processing fails
**Solution**:
1. Check file format compatibility
2. Verify data integrity
3. Review error logs
4. Try smaller batch sizes

### Debugging

#### Enable Debug Logging
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Check Controller Status
```bash
python run_uspto.py --test
```

#### Monitor Database
```sql
-- Check processing status
SELECT * FROM processing_status;

-- Check product tables
SELECT table_name FROM information_schema.tables 
WHERE table_name LIKE 'product_%';

-- Check processing history
SELECT * FROM file_processing_history 
ORDER BY download_date DESC;
```

### Performance Monitoring

#### Monitor Memory Usage
```python
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    print(f"Memory usage: {memory_info.rss / 1024 / 1024:.1f} MB")
```

#### Monitor Database Performance
```sql
-- Check table sizes
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE tablename LIKE 'product_%'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

---

## File Structure

```
uspto_data/
├── zips/                    # Downloaded ZIP files
│   ├── TRCFECO2/
│   │   └── case_file.csv.zip
│   ├── TRASECO/
│   │   └── assignment_data.zip
│   └── TTABTDXF/
│       └── ttab_proceedings.zip
├── extracted/               # Extracted files
│   ├── TRCFECO2/
│   │   └── case_file.csv/
│   │       └── case_file.csv
│   ├── TRASECO/
│   │   └── assignment_data/
│   │       ├── tm_assignee.csv
│   │       └── tm_assignment.csv
│   └── TTABTDXF/
│       └── ttab_proceedings/
│           └── proceedings.xml
├── processed/               # Processed data files
├── checkpoints/             # Processing checkpoints
└── batches/                 # Batch files
    ├── TRCFECO2/
    ├── TRASECO/
    └── TTABTDXF/
```

---

## Best Practices

### Configuration
1. **Start Small**: Begin with small batch sizes and increase gradually
2. **Monitor Resources**: Keep an eye on memory and disk usage
3. **Test First**: Use `--test` flag to verify all controllers work
4. **Backup Data**: Always backup your database before large operations

### Processing
1. **Batch Size**: Use batch sizes between 5,000-10,000 for optimal performance
2. **Memory Limits**: Set memory limits based on your system capabilities
3. **File Limits**: Process 1-2 files per product for initial runs
4. **Error Handling**: Always check results and handle errors appropriately

### Database
1. **Indexes**: Let the system create indexes automatically
2. **Maintenance**: Run VACUUM and ANALYZE regularly
3. **Monitoring**: Monitor table sizes and query performance
4. **Backups**: Regular database backups are essential

---

## Setup and Installation

### Quick Setup

The system includes an automated setup script that handles most of the configuration:

```bash
# Run the setup script
python setup.py

# Install dependencies
pip install -r requirements.txt
```

### Manual Setup

#### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- `requests`: For API interactions
- `pandas`: For data processing
- `psycopg2`: For PostgreSQL database operations
- `lxml`: For XML processing

#### 2. Database Setup

Create the PostgreSQL database and schema:

```bash
# Create database
createdb trademarks

# Run schema creation
psql -U postgres -d trademarks -f create_multi_product_schema.sql
```

#### 3. Configuration

The setup script creates a default `uspto_config.json` file. Edit it to match your environment:

```json
{
  "database": {
    "dbname": "trademarks",
    "user": "postgres",
    "password": "your_password",
    "host": "localhost",
    "port": "5432"
  }
}
```

#### 4. Directory Structure

The system automatically creates the necessary directories:

```
uspto_data/
├── zips/                    # Downloaded ZIP files
├── extracted/               # Extracted files
├── processed/               # Processed data files
├── checkpoints/             # Processing checkpoints
└── batches/                 # Batch files
```

### Verification

Test the installation:

```bash
# Test all controllers
python run_uspto.py --test

# Check database tables
python controllers/utils/check_tables.py
```

### Development Setup

For development and testing:

```bash
# Clone or download the project
# Navigate to project directory

# Run setup
python setup.py

# Install dependencies
pip install -r requirements.txt

# Test installation
python run_uspto.py --test --max-files 1
```

---

## Conclusion

The USPTO Controller-Based Data Processing System provides a robust, scalable solution for processing USPTO trademark data. Its modular architecture allows for easy maintenance, testing, and extension while providing excellent performance and reliability.

The system is designed to handle large datasets efficiently while being friendly to low-spec systems. With proper configuration and monitoring, it can process millions of records reliably and efficiently.

For additional support or questions, refer to the example files and test scripts provided with the system.
